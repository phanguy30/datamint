{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c422b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0 | loss=1.2816 | acc=0.50\n",
      "epoch  20 | loss=0.0855 | acc=1.00\n",
      "epoch  40 | loss=0.0409 | acc=1.00\n",
      "epoch  60 | loss=0.0243 | acc=1.00\n",
      "epoch  80 | loss=0.0166 | acc=1.00\n",
      "epoch 100 | loss=0.0124 | acc=1.00\n",
      "epoch 120 | loss=0.0099 | acc=1.00\n",
      "epoch 140 | loss=0.0082 | acc=1.00\n",
      "epoch 160 | loss=0.0069 | acc=1.00\n",
      "epoch 180 | loss=0.0060 | acc=1.00\n",
      "\n",
      "Final predictions:\n",
      "x=[2.0, 3.0], y_true=0, prob=0.004, pred=0\n",
      "x=[1.0, 1.5], y_true=0, prob=0.005, pred=0\n",
      "x=[2.5, 2.2], y_true=0, prob=0.007, pred=0\n",
      "x=[3.0, 0.5], y_true=1, prob=0.993, pred=1\n",
      "x=[4.0, 1.0], y_true=1, prob=0.995, pred=1\n",
      "x=[3.5, 0.2], y_true=1, prob=0.995, pred=1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from Models.MLP.MLPnetwork import MLPNetwork\n",
    "from Models.MLP.Value import Value\n",
    "from Trainer import Trainer\n",
    "data = [\n",
    "    ([2.0, 3.0], 0),\n",
    "    ([1.0, 1.5], 0),\n",
    "    ([2.5, 2.2], 0),\n",
    "    ([3.0, 0.5], 1),\n",
    "    ([4.0, 1.0], 1),\n",
    "    ([3.5, 0.2], 1),\n",
    "]\n",
    "\n",
    "# Binary Cross-Entropy loss for sigmoid outputs\n",
    "def bce_loss(prob, y_true):\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy Loss for sigmoid outputs.\n",
    "    \n",
    "    prob: Value    # model output after sigmoid (a probability)\n",
    "    y_true: int    # 0 or 1\n",
    "    \"\"\"\n",
    "\n",
    "    # y * log(p)\n",
    "    term1 = prob.log() if y_true == 1 else Value(0.0)\n",
    "\n",
    "    # (1 - y) * log(1 - p)\n",
    "    term2 = (1 - prob).log() if y_true == 0 else Value(0.0)\n",
    "\n",
    "    # BCE = - (term1 + term2)\n",
    "    return -(term1 + term2)\n",
    "    \n",
    "\n",
    "\n",
    "# creating the model with 1 hidden layer and 1 output\n",
    "model = MLPNetwork(\n",
    "    input_dim=2,\n",
    "    n_neurons=[4, 1],          # [hidden_size, output_size]\n",
    "    label=\"toy\",\n",
    "    activation_type=\"tanh\",\n",
    "    classification=\"sigmoid\"\n",
    ")\n",
    "\n",
    "# Training loop (simple SGD)\n",
    "learning_rate = 0.1\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = Value(0.0)\n",
    "    correct = 0\n",
    "\n",
    "    for (x_raw, y_true) in data:\n",
    "        # Forward pass\n",
    "        probs = model.forward(x_raw)\n",
    "        loss = bce_loss(probs[0], y_true)\n",
    "\n",
    "        # Accumulate loss for reporting\n",
    "        total_loss = total_loss + loss\n",
    "\n",
    "        # Accuracy\n",
    "        pred = 1 if probs[0].data > 0.5 else 0\n",
    "        if pred == y_true:\n",
    "            correct += 1\n",
    "\n",
    "        # Backward + update (SGD step)\n",
    "        model.zero_grad()     # reset grads BEFORE backward\n",
    "        loss.backward()\n",
    "        for p in model.parameters():\n",
    "            p.data -= learning_rate * p.grad\n",
    "\n",
    "    # Report average loss + accuracy\n",
    "    if epoch % 20 == 0:\n",
    "        avg_loss = total_loss.data / len(data)\n",
    "        acc = correct / len(data)\n",
    "        print(f\"epoch {epoch:3d} | loss={avg_loss:.4f} | acc={acc:.2f}\")\n",
    "\n",
    "# test\n",
    "print(\"\\nFinal predictions:\")\n",
    "for (x_raw, y_true) in data:\n",
    "    probs = model.forward(x_raw)\n",
    "    pred = 1 if probs[0].data > 0.5 else 0\n",
    "    print(f\"x={x_raw}, y_true={y_true}, prob={probs[0].data:.3f}, pred={pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e1230b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized for regression task using LinearLoss loss.\n",
      "Epoch 1/100, Loss: 3.1873\n",
      "Epoch 20/100, Loss: 0.0686\n",
      "Epoch 40/100, Loss: 0.0203\n",
      "Epoch 60/100, Loss: 0.0099\n",
      "Epoch 80/100, Loss: 0.0063\n",
      "Epoch 100/100, Loss: 0.0046\n",
      "Test Loss: 0.0044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004350684529580903"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Models.MLP import MLPNetwork\n",
    "from Trainer.trainer import Trainer\n",
    "data = [\n",
    "    ([2.0, 3.0], [0,1]),\n",
    "    ([1.0, 1.5], [0,1]),\n",
    "    ([2.5, 2.2], [0,1]),\n",
    "    ([3.0, 0.5], [0,1]),\n",
    "    ([4.0, 1.0], [0,1]),\n",
    "    ([3.5, 0.2], [0,1]),\n",
    "]\n",
    "predictors = []\n",
    "labels = []\n",
    "\n",
    "for x,y in data:\n",
    "    predictors.append(x)\n",
    "    labels.append(y)\n",
    "\n",
    "model = MLPNetwork(input_dim=2, n_neurons=[4, 2], label=\"toy\", activation_type=\"tanh\", classification=\"none\")\n",
    "trainer = Trainer(model=model, epochs=100)\n",
    "trainer.fit(predictors, labels)\n",
    "trainer.test(predictors, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b9af505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def make_toy_data(n_per_class=100):\n",
    "    X_real = []\n",
    "    y_labels = []\n",
    "\n",
    "    for _ in range(n_per_class):\n",
    "        # class 0: around (-1, -1)\n",
    "        x1 = -1 + random.gauss(0, 0.2)\n",
    "        x2 = -1 + random.gauss(0, 0.2)\n",
    "        X_real.append([x1, x2])\n",
    "        y_labels.append(0)\n",
    "\n",
    "    for _ in range(n_per_class):\n",
    "        # class 1: around (1, 1)\n",
    "        x1 = 1 + random.gauss(0, 0.2)\n",
    "        x2 = 1 + random.gauss(0, 0.2)\n",
    "        X_real.append([x1, x2])\n",
    "        y_labels.append(1)\n",
    "\n",
    "    return X_real, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1eb601e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized for regression task using LinearLoss loss.\n"
     ]
    }
   ],
   "source": [
    "from Models.MLPGenerator import MLPGenerator\n",
    "from Trainer.trainer import Trainer\n",
    "import random\n",
    "\n",
    "def make_toy_data(n_per_class=100):\n",
    "    X_real = []\n",
    "    y_labels = []\n",
    "\n",
    "    for _ in range(n_per_class):\n",
    "        # class 0: around (-1, -1)\n",
    "        x1 = -1 + random.gauss(0, 0.2)\n",
    "        x2 = -1 + random.gauss(0, 0.2)\n",
    "        X_real.append([x1, x2])\n",
    "        y_labels.append(0)\n",
    "\n",
    "    for _ in range(n_per_class):\n",
    "        # class 1: around (1, 1)\n",
    "        x1 = 1 + random.gauss(0, 0.2)\n",
    "        x2 = 1 + random.gauss(0, 0.2)\n",
    "        X_real.append([x1, x2])\n",
    "        y_labels.append(1)\n",
    "\n",
    "    return X_real, y_labels\n",
    "\n",
    "def to_one_hot(label, num_classes):\n",
    "    return [1 if i == label else 0 for i in range(num_classes)]\n",
    "\n",
    "def build_generator_training_data(X_real, y_labels, latent_dim, num_classes):\n",
    "    X_train = []   # inputs to generator\n",
    "    y_train = []   # targets (real x)\n",
    "\n",
    "    for x, y in zip(X_real, y_labels):\n",
    "        # sample random noise z\n",
    "        z = [random.uniform(-1, 1) for _ in range(latent_dim)]\n",
    "        # one-hot encode label\n",
    "        y_vec = to_one_hot(y, num_classes)\n",
    "        # concat [z, y_onehot] as input\n",
    "        inp = z + y_vec\n",
    "\n",
    "        X_train.append(inp)\n",
    "        y_train.append(x)\n",
    "\n",
    "    return X_train, y_train\n",
    "# hyperparameters\n",
    "latent_dim  = 4          # size of noise vector z\n",
    "num_classes = 2          # y in {0, 1}\n",
    "cond_dim    = num_classes\n",
    "output_dim  = 2          # x is 2D\n",
    "hidden_sizes = [16, 16]  # small MLP\n",
    "\n",
    "# create generator model\n",
    "gen_model = MLPGenerator(\n",
    "    latent_dim=latent_dim,\n",
    "    cond_dim=cond_dim,\n",
    "    output_dim=output_dim,\n",
    "    hidden_sizes=hidden_sizes,\n",
    "    activation_type=\"tanh\"\n",
    ")\n",
    "\n",
    "# create trainer (regression, so classification=\"none\" in model)\n",
    "trainer = Trainer(\n",
    "    model=gen_model,\n",
    "    optimizer=None,      # uses default SGD in your __init__\n",
    "    loss_fn=None,        # uses default LinearLoss in your __init__\n",
    "    epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c691b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.2782\n",
      "Epoch 20/50, Loss: 0.0442\n",
      "Epoch 40/50, Loss: 0.0426\n"
     ]
    }
   ],
   "source": [
    "# 1. real data\n",
    "X_real, y_labels = make_toy_data(n_per_class=200)\n",
    "\n",
    "# 2. build (input, target) pairs for generator\n",
    "X_train, y_train = build_generator_training_data(\n",
    "    X_real, y_labels,\n",
    "    latent_dim=latent_dim,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# 3. train\n",
    "trainer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d7d60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic samples for class 0:\n",
      "[Value: -1.0559758216501918, Grad: 0, Value: -0.9542930327942515, Grad: 0]\n",
      "[Value: -0.9809952181716437, Grad: 0, Value: -1.0358120493219025, Grad: 0]\n",
      "[Value: -1.0725440286621213, Grad: 0, Value: -0.9512467152491785, Grad: 0]\n",
      "[Value: -1.0064335722720055, Grad: 0, Value: -1.0117034090288402, Grad: 0]\n",
      "[Value: -0.9000556539326645, Grad: 0, Value: -1.049317561478556, Grad: 0]\n",
      "\n",
      "Synthetic samples for class 1:\n",
      "[Value: 0.9608756327356689, Grad: 0, Value: 0.9623279563241465, Grad: 0]\n",
      "[Value: 1.0562431388054598, Grad: 0, Value: 0.9640717531464561, Grad: 0]\n",
      "[Value: 1.014131203869769, Grad: 0, Value: 0.9649806802962615, Grad: 0]\n",
      "[Value: 1.1632670713306237, Grad: 0, Value: 0.8835455789731771, Grad: 0]\n",
      "[Value: 1.0035622775327868, Grad: 0, Value: 0.9617033974953391, Grad: 0]\n"
     ]
    }
   ],
   "source": [
    "def sample_z(latent_dim):\n",
    "    return [random.uniform(-1, 1) for _ in range(latent_dim)]\n",
    "\n",
    "# generate 5 new samples for class 0\n",
    "print(\"Synthetic samples for class 0:\")\n",
    "for _ in range(5):\n",
    "    z = sample_z(latent_dim)\n",
    "    y_vec = to_one_hot(0, num_classes)      # condition on class 0\n",
    "    x_hat = gen_model.generate(z, y_vec)\n",
    "    print(x_hat)\n",
    "\n",
    "# generate 5 new samples for class 1\n",
    "print(\"\\nSynthetic samples for class 1:\")\n",
    "for _ in range(5):\n",
    "    z = sample_z(latent_dim)\n",
    "    y_vec = to_one_hot(1, num_classes)      # condition on class 1\n",
    "    x_hat = gen_model.generate(z, y_vec)\n",
    "    print(x_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
